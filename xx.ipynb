{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " image (InputLayer)          [(None, 32, 128, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 32, 128, 32)          320       ['image[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 16, 64, 32)           0         ['conv2d_14[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 16, 64, 64)           18496     ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 8, 32, 64)            0         ['conv2d_15[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 8, 32, 128)           73856     ['max_pooling2d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 8, 32, 128)           147584    ['conv2d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 8, 32, 256)           295168    ['conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 8, 32, 256)           1024      ['conv2d_18[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 8, 32, 256)           590080    ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 8, 32, 256)           1024      ['conv2d_19[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPoolin  (None, 4, 32, 256)           0         ['batch_normalization_5[0][0]'\n",
      " g2D)                                                               ]                             \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)          (None, 4, 32, 64)            65600     ['max_pooling2d_8[0][0]']     \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 256, 32)              0         ['conv2d_20[0][0]']           \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirecti  [(None, 256, 128),           164864    ['reshape[0][0]']             \n",
      " onal)                        (None, 128),                                                        \n",
      "                              (None, 128),                                                        \n",
      "                              (None, 128),                                                        \n",
      "                              (None, 128)]                                                        \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirecti  (None, 256, 128)             263168    ['bidirectional_4[0][0]',     \n",
      " onal)                                                               'bidirectional_4[0][1]',     \n",
      "                                                                     'bidirectional_4[0][2]',     \n",
      "                                                                     'bidirectional_4[0][3]',     \n",
      "                                                                     'bidirectional_4[0][4]']     \n",
      "                                                                                                  \n",
      " label (InputLayer)          [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 256, 112)             14448     ['bidirectional_5[0][0]']     \n",
      "                                                                                                  \n",
      " ctc_loss (CTCLayer)         (None, 256, 112)             0         ['label[0][0]',               \n",
      "                                                                     'dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1635632 (6.24 MB)\n",
      "Trainable params: 1634608 (6.24 MB)\n",
      "Non-trainable params: 1024 (4.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Baramullah\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from flask import Flask, request, jsonify\n",
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import fastwer\n",
    "import jiwer\n",
    "from jiwer import wer\n",
    "from jiwer import cer\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "#from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from itertools import groupby\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Path to the Tesseract executable\n",
    "# pytesseract.pytesseract.tesseract_cmd = \"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\"\n",
    "\n",
    "characters_train = [' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¢', '£', '¥', '§', '®', '°', 'é', '—', '‘', '’', '“', '”', '€', '™', 'ﬁ', 'ﬂ']\n",
    "characters = characters_train \n",
    "\n",
    "\n",
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, name=None):\n",
    "\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        return y_pred\n",
    "\n",
    "def ctc_decoder(predictions):\n",
    "    '''\n",
    "    input: given batch of predictions from text rec model\n",
    "    output: return lists of raw extracted text\n",
    "\n",
    "    '''\n",
    "    text_list = []\n",
    "    \n",
    "    pred_indcies = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    for i in range(pred_indcies.shape[0]):\n",
    "        ans = \"\"\n",
    "        \n",
    "        ## merge repeats\n",
    "        merged_list = [k for k,_ in groupby(pred_indcies[i])]\n",
    "        \n",
    "        ## remove blanks\n",
    "        for p in merged_list:\n",
    "            if p != len(characters):\n",
    "                ans +=characters[int(p)]\n",
    "        \n",
    "        text_list.append(ans)\n",
    "        \n",
    "    return text_list\n",
    "\n",
    "def create_inference_model(training_model):\n",
    "    # Extract the layers till softmax output from the training model\n",
    "    inference_model = Model(inputs=training_model.get_layer(name=\"image\").input,\n",
    "                            outputs=training_model.get_layer(name=\"dense\").output)\n",
    "    \n",
    "    return inference_model\n",
    "\n",
    "\n",
    "custom_objects = {'CTCLayer': CTCLayer}\n",
    "custom_objects_1 = {}\n",
    "\n",
    "# Load the model with custom objects\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model_new = load_model('text_recognition.h5')\n",
    "\n",
    "print(model_new.summary())\n",
    "\n",
    "chart_recog_model = load_model('model.h5')\n",
    "\n",
    "inference_model = create_inference_model(model_new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_single_sample(img_path):\n",
    "    img_width = 128\n",
    "    img_height = 32\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.io.decode_png(img, channels=1)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, [img_height, img_width])\n",
    "    return img\n",
    "\n",
    "def predText(img): \n",
    "    # Read characters from the text file\n",
    "    image = np.expand_dims(img, axis=0)  # Add a batch dimension\n",
    "\n",
    "    # Run prediction\n",
    "    preds = inference_model.predict(image)\n",
    "\n",
    "    # Decode CTC output to text\n",
    "    input_len = np.ones(preds.shape[0]) * preds.shape[1]\n",
    "    # Uses greedy search. For complex tasks, you can use beam search\n",
    "    decoded_preds, _ = K.ctc_decode(preds, input_length=input_len, greedy=True)\n",
    "    decoded_preds = decoded_preds[0][0]  # only interested in the first result\n",
    "\n",
    "    # Convert to string\n",
    "    out = ''\n",
    "    for i in range(decoded_preds.shape[0]):\n",
    "        c = tf.keras.backend.get_value(decoded_preds[i])\n",
    "        if c < len(characters_train):  # Ensure index doesn't exceed characters_train length\n",
    "            out += characters_train[c]\n",
    "\n",
    "    substr = 'ﬂ'\n",
    "    loc = out.find(substr)\n",
    "    out = out[:loc]\n",
    "\n",
    "    return out\n",
    "\n",
    "def correct_skew(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    # Threshold the image\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "   \n",
    "    # Use Hough Transform to detect lines\n",
    "    lines = cv2.HoughLinesP(thresh, 1, np.pi / 180, threshold=50, minLineLength=50, maxLineGap=10)\n",
    "    #print(lines)\n",
    "    if(lines is None):\n",
    "#        print('hi')\n",
    "        return image\n",
    "    angles = []\n",
    "    for line in lines:\n",
    "        x1, y1, x2, y2 = line[0]\n",
    "        angle = np.arctan2(y2 - y1, x2 - x1) * 180.0 / np.pi\n",
    "        angles.append(angle)\n",
    "       \n",
    "    # Compute median angle\n",
    "    median_angle = np.median(angles)\n",
    "   # print(f\"Median angle: {median_angle}\")\n",
    "    if(median_angle>30 or median_angle<-30):  \n",
    "        (h, w) = image.shape[:2]\n",
    "        # Rotate image to correct skew\n",
    "        #(h, w) = image.shape[:2]\n",
    "        center = (w // 2, h // 2)\n",
    "        # Calculate new bounding dimensions\n",
    "        alpha = np.abs(angle) * np.pi / 180.0\n",
    "        bound_w = int(h * np.abs(np.sin(alpha)) + w * np.abs(np.cos(alpha)))\n",
    "        bound_h = int(h * np.abs(np.cos(alpha)) + w * np.abs(np.sin(alpha)))\n",
    "\n",
    "        # Adjust the rotation matrix\n",
    "        M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        M[0, 2] += (bound_w - w) // 2\n",
    "        M[1, 2] += (bound_h - h) // 2\n",
    "\n",
    "        # Perform the rotation\n",
    "        rotated = cv2.warpAffine(image, M, (bound_w, bound_h), flags=cv2.INTER_CUBIC)\n",
    "\n",
    "        return rotated\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "def obtain_recog(img):\n",
    "    im1=correct_skew(img)\n",
    "    cv2.imwrite('as.png',im1)\n",
    "    img=process_single_sample('as.png')\n",
    "    pred_str=predText(img)\n",
    "    return pred_str\n",
    "\n",
    "\n",
    "def get_latest_exp_folder(exp_folder):\n",
    "    exp_folders = [os.path.join(exp_folder, d) for d in os.listdir(exp_folder) if os.path.isdir(os.path.join(exp_folder, d))]\n",
    "    latest_exp_folder = max(exp_folders, key=os.path.getmtime)\n",
    "    return latest_exp_folder\n",
    "\n",
    "def apply_ocr(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    text=obtain_recog(img)\n",
    "    return text\n",
    "\n",
    "\n",
    "def getchartcat(filepath):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    print(\"in getch\")\n",
    "    mod = chart_recog_model\n",
    "    display_labels1=['area','heatmap','horizontal_bar','horizontal_interval','line','manhattan','map','pie','scatter','scatter-line','surface','venn','vertical_bar','vertical_box','vertical_interval']\n",
    "\n",
    "\n",
    "    def preprocess_image(image):\n",
    "        # Resize image to match model input shape\n",
    "        resized_image = image.resize((224, 224))\n",
    "        # Convert image to numpy array\n",
    "        img_array = np.asarray(resized_image)\n",
    "        # Normalize pixel values to range [0, 1]\n",
    "        img_array = img_array / 255.0\n",
    "        # Expand dimensions to match model input shape\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        return img_array\n",
    "\n",
    "    img = Image.open(filepath)\n",
    "    img_array = preprocess_image(img)\n",
    "    print(\"here\")\n",
    "    # Perform inference using the model\n",
    "    predictions = mod.predict(img_array)\n",
    "    predictions_list = predictions.tolist()\n",
    "    # print(predictions_list)\n",
    "    predictions_array = np.array(predictions_list)\n",
    "\n",
    "    print(predictions_array)\n",
    "\n",
    "    # Find the index of the maximum probability\n",
    "    max_prob_index = np.argmax(predictions_array)\n",
    "    predicted_label = display_labels1[max_prob_index]\n",
    "\n",
    "    print(\"predicted: \", predicted_label)\n",
    "    for label, probability in zip(display_labels1, predictions_array[0]):\n",
    "        print(f\"{label}: {probability}\")\n",
    "    return predicted_label\n",
    "\n",
    "\n",
    "image_path = \"./4.jpg\"\n",
    "text = apply_ocr(image_path)\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloTRC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
